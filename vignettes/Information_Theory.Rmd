---
title: "Information Theory"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Information Theory}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

## Information Theory measures in `philentropy`

> The laws of probability, so true in general, so fallacious in particular.
> 
> \- Edward Gibbon

Information theory and statistics were beautifully fused by `Solomon Kullback`. This fusion allowed to quantify correlations and similarities between random variables
using a more sophisticated toolkit. Modern fields such as machine learning and statistical data science build upon this fusion and the most powerful statistical techniques used today are based on an information theoretic foundation.

The `philentropy` aims to follow this tradition and therefore, it implements
the most important information theory measures.

### Shannon's Entropy H(X)

> $H(X) = -\sum\limits_{i=1}^n P(x_i) * log_b(P(x_i))$

```r
# define probabilities P(X)
Prob <- 1:10/sum(1:10)
# Compute Shannon's Entropy
H(Prob)
```

```
[1] 3.103643
```

### Shannon's Joint-Entropy H(X,Y)

> $H(X,Y) = -\sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b(P(x_i, y_j))$

```r
# define the joint distribution P(X,Y)
P_xy <- 1:100/sum(1:100)
# Compute Shannon's Joint-Entropy
JE(P_xy)
```

```
[1] 6.372236
```

### Shannon's Conditional-Entropy H(X | Y)

> $H(Y|X) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b( P(x_i) / P(x_i, y_j) )$

```r
# define the distribution P(X)
P_x <- 1:10/sum(1:10)
# define the distribution P(Y)
P_y <- 1:10/sum(1:10)

# Compute Shannon's Joint-Entropy
CE(P_x, P_y)
```

```
[1] 0
```

### Mutual Information I(X,Y)

> $MI(X,Y) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b( P(x_i, y_j) / ( P(x_i) * P(y_j) )$


```r
# define the distribution P(X)
P_x <- 1:10/sum(1:10)
# define the distribution P(Y)
P_y <- 20:29/sum(20:29)
# define the joint-distribution P(X,Y)
P_xy <- 1:10/sum(1:10)

# Compute Shannon's Joint-Entropy
MI(P_x, P_y, P_xy)
```

```
[1] 3.311973
```


### Kullback-Leibler Divergence

> $KL(P || Q) = \sum\limits_{i=1}^n P(p_i) * log_2(P(p_i) / P(q_i)) = H(P, Q) - H(P)$

where `H(P, Q)` denotes the joint entropy of the probability distributions `P` and `Q` and `H(P)` denotes the entropy of probability distribution `P`. In case `P = Q` then `KL(P, Q) = 0` and in case `P != Q` then `KL(P, Q) > 0`.

The KL divergence is a non-symmetric measure of the directed divergence between two probability distributions P and Q. It only fulfills the positivity property of a distance metric.

Because of the relation `KL(P||Q) = H(P,Q) - H(P)`, the Kullback-Leibler divergence of two probability distributions `P` and `Q` is also named `Cross Entropy` of two probability distributions `P` and `Q`.

```r
# Kulback-Leibler Divergence between random variables P and Q
P <- 1:10/sum(1:10)
Q <- 20:29/sum(20:29)
x <- rbind(P,Q)

# Kulback-Leibler Divergence between P and Q using different log bases
KL(x, unit = "log2") # Default
KL(x, unit = "log")
KL(x, unit = "log10")
```

```
# KL(x, unit = "log2") # Default
Kulback-Leibler Divergence using unit 'log2'.
kullback-leibler 
       0.1392629 
# KL(x, unit = "log")
Kulback-Leibler Divergence using unit 'log'.
kullback-leibler 
      0.09652967 
# KL(x, unit = "log10")
Kulback-Leibler Divergence using unit 'log10'.
kullback-leibler 
       0.0419223 
```

### Jensen-Shannon Divergence

This function computes the `Jensen-Shannon Divergence` `JSD(P || Q)` between two probability distributions `P` and `Q` with equal weights `π_1` = `π_2` = 1/2.

The Jensen-Shannon Divergence JSD(P || Q) between two probability distributions P and Q is defined as:

> $JSP(P || Q) = 0.5 * (KL(P || R) + KL(Q || R))$

where `R = 0.5 * (P + Q)` denotes the mid-point of the probability vectors `P` and `Q`, and `KL(P || R)`, `KL(Q || R)` denote the `Kullback-Leibler Divergence` of `P` and `R`, as well as `Q` and `R`.

```r
# Jensen-Shannon Divergence between P and Q
P <- 1:10/sum(1:10)
Q <- 20:29/sum(20:29)
x <- rbind(P,Q)

# Jensen-Shannon Divergence between P and Q using different log bases
JSD(x, unit = "log2") # Default
JSD(x, unit = "log")
JSD(x, unit = "log10")
```

```
# JSD(x, unit = "log2") # Default
Jensen-Shannon Divergence using unit 'log2'.
jensen-shannon 
    0.03792749 
# JSD(x, unit = "log")
Jensen-Shannon Divergence using unit 'log'.
jensen-shannon 
    0.02628933 
# JSD(x, unit = "log10")
Jensen-Shannon Divergence using unit 'log10'.
jensen-shannon 
    0.01141731 
```

Alternatively, users can specify count data.

```r
# Jensen-Shannon Divergence Divergence between count vectors P.count and Q.count
P.count <- 1:10
Q.count <- 20:29
x.count <- rbind(P.count, Q.count)

JSD(x, est.prob = "empirical")
```

```
Jensen-Shannon Divergence using unit 'log2'.
jensen-shannon 
    0.03792749
```

Or users can compute distances based on a probability matrix

```r
# Example: Distance Matrix using JSD-Distance
Prob <- cbind(1:10/sum(1:10), 20:29/sum(20:29), 30:39/sum(30:39))

# compute the KL matrix of a given probability matrix
JSDMatrix <- JSD(Prob)

JSDMatrix
```

```
             v1          v2           v3           v4           v5           v6           v7           v8           v9          v10
v1  0.000000000 0.002281344 0.0070703292 0.0130974082 0.0198872770 0.0272064915 0.0349217336 0.0429489730 0.0512313857 0.0597284553
v2  0.002281344 0.000000000 0.0013712612 0.0046558178 0.0091499588 0.0144915157 0.0204673435 0.0269404448 0.0338170292 0.0410297463
v3  0.007070329 0.001371261 0.0000000000 0.0009888288 0.0035053122 0.0071074548 0.0115291444 0.0165964176 0.0221886494 0.0282183873
v4  0.013097408 0.004655818 0.0009888288 0.0000000000 0.0007770690 0.0028250735 0.0058409350 0.0096238555 0.0140331082 0.0189657868
v5  0.019887277 0.009149959 0.0035053122 0.0007770690 0.0000000000 0.0006421763 0.0023736851 0.0049736265 0.0082858198 0.0121952203
v6  0.027206491 0.014491516 0.0071074548 0.0028250735 0.0006421763 0.0000000000 0.0005485296 0.0020514117 0.0043403270 0.0072905195
v7  0.034921734 0.020467343 0.0115291444 0.0058409350 0.0023736851 0.0005485296 0.0000000000 0.0004796040 0.0018093113 0.0038564737
v8  0.042948973 0.026940445 0.0165964176 0.0096238555 0.0049736265 0.0020514117 0.0004796040 0.0000000000 0.0004266772 0.0016204852
v9  0.051231386 0.033817029 0.0221886494 0.0140331082 0.0082858198 0.0043403270 0.0018093113 0.0004266772 0.0000000000 0.0003847076
v10 0.059728455 0.041029746 0.0282183873 0.0189657868 0.0121952203 0.0072905195 0.0038564737 0.0016204852 0.0003847076 0.0000000000
```

#### General properties of the `Jensen-Shannon Divergence`:

- JSD is non-negative.

- JSD is a symmetric measure JSD(P || Q) = JSD(Q || P).

- JSD = 0, if and only if P = Q.


### Generalized Jensen-Shannon Divergence


```r
# generate example probability matrix
Prob <- cbind(1:10/sum(1:10), 20:29/sum(20:29), 30:39/sum(30:39))

# compute the Generalized JSD comparing the PS probability matrix
gJSD(Prob)
```

```
[1] 0.03512892
```

