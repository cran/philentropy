<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">



<title>Information Theory</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Information Theory</h1>



<div id="information-theory-measures-in-philentropy" class="section level2">
<h2>Information Theory measures in <code>philentropy</code></h2>
<blockquote>
<p>The laws of probability, so true in general, so fallacious in particular.</p>
<p>- Edward Gibbon</p>
</blockquote>
<p>Information theory and statistics were beautifully fused by <code>Solomon Kullback</code>. This fusion allowed to quantify correlations and similarities between random variables using a more sophisticated toolkit. Modern fields such as machine learning and statistical data science build upon this fusion and the most powerful statistical techniques used today are based on an information theoretic foundation.</p>
<p>The <code>philentropy</code> aims to follow this tradition and therefore, it implements the most important information theory measures.</p>
<div id="shannons-entropy-hx" class="section level3">
<h3>Shannon’s Entropy H(X)</h3>
<blockquote>
<p><span class="math inline">\(H(X) = -\sum\limits_{i=1}^n P(x_i) * log_b(P(x_i))\)</span></p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define probabilities P(X)</span>
Prob &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
<span class="co"># Compute Shannon's Entropy</span>
<span class="kw">H</span>(Prob)</code></pre></div>
<pre><code>[1] 3.103643</code></pre>
</div>
<div id="shannons-joint-entropy-hxy" class="section level3">
<h3>Shannon’s Joint-Entropy H(X,Y)</h3>
<blockquote>
<p><span class="math inline">\(H(X,Y) = -\sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b(P(x_i, y_j))\)</span></p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define the joint distribution P(X,Y)</span>
P_xy &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>)
<span class="co"># Compute Shannon's Joint-Entropy</span>
<span class="kw">JE</span>(P_xy)</code></pre></div>
<pre><code>[1] 6.372236</code></pre>
</div>
<div id="shannons-conditional-entropy-hx-y" class="section level3">
<h3>Shannon’s Conditional-Entropy H(X | Y)</h3>
<blockquote>
<p><span class="math inline">\(H(Y|X) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b( P(x_i) / P(x_i, y_j) )\)</span></p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define the distribution P(X)</span>
P_x &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
<span class="co"># define the distribution P(Y)</span>
P_y &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)

<span class="co"># Compute Shannon's Joint-Entropy</span>
<span class="kw">CE</span>(P_x, P_y)</code></pre></div>
<pre><code>[1] 0</code></pre>
</div>
<div id="mutual-information-ixy" class="section level3">
<h3>Mutual Information I(X,Y)</h3>
<blockquote>
<p><span class="math inline">\(MI(X,Y) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b( P(x_i, y_j) / ( P(x_i) * P(y_j) )\)</span></p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define the distribution P(X)</span>
P_x &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
<span class="co"># define the distribution P(Y)</span>
P_y &lt;-<span class="st"> </span><span class="dv">20</span><span class="op">:</span><span class="dv">29</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">20</span><span class="op">:</span><span class="dv">29</span>)
<span class="co"># define the joint-distribution P(X,Y)</span>
P_xy &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)

<span class="co"># Compute Shannon's Joint-Entropy</span>
<span class="kw">MI</span>(P_x, P_y, P_xy)</code></pre></div>
<pre><code>[1] 3.311973</code></pre>
</div>
<div id="kullback-leibler-divergence" class="section level3">
<h3>Kullback-Leibler Divergence</h3>
<blockquote>
<p><span class="math inline">\(KL(P || Q) = \sum\limits_{i=1}^n P(p_i) * log_2(P(p_i) / P(q_i)) = H(P, Q) - H(P)\)</span></p>
</blockquote>
<p>where <code>H(P, Q)</code> denotes the joint entropy of the probability distributions <code>P</code> and <code>Q</code> and <code>H(P)</code> denotes the entropy of probability distribution <code>P</code>. In case <code>P = Q</code> then <code>KL(P, Q) = 0</code> and in case <code>P != Q</code> then <code>KL(P, Q) &gt; 0</code>.</p>
<p>The KL divergence is a non-symmetric measure of the directed divergence between two probability distributions P and Q. It only fulfills the positivity property of a distance metric.</p>
<p>Because of the relation <code>KL(P||Q) = H(P,Q) - H(P)</code>, the Kullback-Leibler divergence of two probability distributions <code>P</code> and <code>Q</code> is also named <code>Cross Entropy</code> of two probability distributions <code>P</code> and <code>Q</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Kulback-Leibler Divergence between random variables P and Q</span>
P &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
Q &lt;-<span class="st"> </span><span class="dv">20</span><span class="op">:</span><span class="dv">29</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">20</span><span class="op">:</span><span class="dv">29</span>)
x &lt;-<span class="st"> </span><span class="kw">rbind</span>(P,Q)

<span class="co"># Kulback-Leibler Divergence between P and Q using different log bases</span>
<span class="kw">KL</span>(x, <span class="dt">unit =</span> <span class="st">&quot;log2&quot;</span>) <span class="co"># Default</span>
<span class="kw">KL</span>(x, <span class="dt">unit =</span> <span class="st">&quot;log&quot;</span>)
<span class="kw">KL</span>(x, <span class="dt">unit =</span> <span class="st">&quot;log10&quot;</span>)</code></pre></div>
<pre><code># KL(x, unit = &quot;log2&quot;) # Default
Kulback-Leibler Divergence using unit 'log2'.
kullback-leibler 
       0.1392629 
# KL(x, unit = &quot;log&quot;)
Kulback-Leibler Divergence using unit 'log'.
kullback-leibler 
      0.09652967 
# KL(x, unit = &quot;log10&quot;)
Kulback-Leibler Divergence using unit 'log10'.
kullback-leibler 
       0.0419223 </code></pre>
</div>
<div id="jensen-shannon-divergence" class="section level3">
<h3>Jensen-Shannon Divergence</h3>
<p>This function computes the <code>Jensen-Shannon Divergence</code> <code>JSD(P || Q)</code> between two probability distributions <code>P</code> and <code>Q</code> with equal weights <code>π_1</code> = <code>π_2</code> = 1/2.</p>
<p>The Jensen-Shannon Divergence JSD(P || Q) between two probability distributions P and Q is defined as:</p>
<blockquote>
<p><span class="math inline">\(JSP(P || Q) = 0.5 * (KL(P || R) + KL(Q || R))\)</span></p>
</blockquote>
<p>where <code>R = 0.5 * (P + Q)</code> denotes the mid-point of the probability vectors <code>P</code> and <code>Q</code>, and <code>KL(P || R)</code>, <code>KL(Q || R)</code> denote the <code>Kullback-Leibler Divergence</code> of <code>P</code> and <code>R</code>, as well as <code>Q</code> and <code>R</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Jensen-Shannon Divergence between P and Q</span>
P &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
Q &lt;-<span class="st"> </span><span class="dv">20</span><span class="op">:</span><span class="dv">29</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">20</span><span class="op">:</span><span class="dv">29</span>)
x &lt;-<span class="st"> </span><span class="kw">rbind</span>(P,Q)

<span class="co"># Jensen-Shannon Divergence between P and Q using different log bases</span>
<span class="kw">JSD</span>(x, <span class="dt">unit =</span> <span class="st">&quot;log2&quot;</span>) <span class="co"># Default</span>
<span class="kw">JSD</span>(x, <span class="dt">unit =</span> <span class="st">&quot;log&quot;</span>)
<span class="kw">JSD</span>(x, <span class="dt">unit =</span> <span class="st">&quot;log10&quot;</span>)</code></pre></div>
<pre><code># JSD(x, unit = &quot;log2&quot;) # Default
Jensen-Shannon Divergence using unit 'log2'.
jensen-shannon 
    0.03792749 
# JSD(x, unit = &quot;log&quot;)
Jensen-Shannon Divergence using unit 'log'.
jensen-shannon 
    0.02628933 
# JSD(x, unit = &quot;log10&quot;)
Jensen-Shannon Divergence using unit 'log10'.
jensen-shannon 
    0.01141731 </code></pre>
<p>Alternatively, users can specify count data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Jensen-Shannon Divergence Divergence between count vectors P.count and Q.count</span>
P.count &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span>
Q.count &lt;-<span class="st"> </span><span class="dv">20</span><span class="op">:</span><span class="dv">29</span>
x.count &lt;-<span class="st"> </span><span class="kw">rbind</span>(P.count, Q.count)

<span class="kw">JSD</span>(x, <span class="dt">est.prob =</span> <span class="st">&quot;empirical&quot;</span>)</code></pre></div>
<pre><code>Jensen-Shannon Divergence using unit 'log2'.
jensen-shannon 
    0.03792749</code></pre>
<p>Or users can compute distances based on a probability matrix</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Example: Distance Matrix using JSD-Distance</span>
Prob &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>), <span class="dv">20</span><span class="op">:</span><span class="dv">29</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">20</span><span class="op">:</span><span class="dv">29</span>), <span class="dv">30</span><span class="op">:</span><span class="dv">39</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">30</span><span class="op">:</span><span class="dv">39</span>))

<span class="co"># compute the KL matrix of a given probability matrix</span>
JSDMatrix &lt;-<span class="st"> </span><span class="kw">JSD</span>(Prob)

JSDMatrix</code></pre></div>
<pre><code>             v1          v2           v3           v4           v5           v6           v7           v8           v9          v10
v1  0.000000000 0.002281344 0.0070703292 0.0130974082 0.0198872770 0.0272064915 0.0349217336 0.0429489730 0.0512313857 0.0597284553
v2  0.002281344 0.000000000 0.0013712612 0.0046558178 0.0091499588 0.0144915157 0.0204673435 0.0269404448 0.0338170292 0.0410297463
v3  0.007070329 0.001371261 0.0000000000 0.0009888288 0.0035053122 0.0071074548 0.0115291444 0.0165964176 0.0221886494 0.0282183873
v4  0.013097408 0.004655818 0.0009888288 0.0000000000 0.0007770690 0.0028250735 0.0058409350 0.0096238555 0.0140331082 0.0189657868
v5  0.019887277 0.009149959 0.0035053122 0.0007770690 0.0000000000 0.0006421763 0.0023736851 0.0049736265 0.0082858198 0.0121952203
v6  0.027206491 0.014491516 0.0071074548 0.0028250735 0.0006421763 0.0000000000 0.0005485296 0.0020514117 0.0043403270 0.0072905195
v7  0.034921734 0.020467343 0.0115291444 0.0058409350 0.0023736851 0.0005485296 0.0000000000 0.0004796040 0.0018093113 0.0038564737
v8  0.042948973 0.026940445 0.0165964176 0.0096238555 0.0049736265 0.0020514117 0.0004796040 0.0000000000 0.0004266772 0.0016204852
v9  0.051231386 0.033817029 0.0221886494 0.0140331082 0.0082858198 0.0043403270 0.0018093113 0.0004266772 0.0000000000 0.0003847076
v10 0.059728455 0.041029746 0.0282183873 0.0189657868 0.0121952203 0.0072905195 0.0038564737 0.0016204852 0.0003847076 0.0000000000</code></pre>
<div id="general-properties-of-the-jensen-shannon-divergence" class="section level4">
<h4>General properties of the <code>Jensen-Shannon Divergence</code>:</h4>
<ul>
<li><p>JSD is non-negative.</p></li>
<li><p>JSD is a symmetric measure JSD(P || Q) = JSD(Q || P).</p></li>
<li><p>JSD = 0, if and only if P = Q.</p></li>
</ul>
</div>
</div>
<div id="generalized-jensen-shannon-divergence" class="section level3">
<h3>Generalized Jensen-Shannon Divergence</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate example probability matrix</span>
Prob &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>), <span class="dv">20</span><span class="op">:</span><span class="dv">29</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">20</span><span class="op">:</span><span class="dv">29</span>), <span class="dv">30</span><span class="op">:</span><span class="dv">39</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">30</span><span class="op">:</span><span class="dv">39</span>))

<span class="co"># compute the Generalized JSD comparing the PS probability matrix</span>
<span class="kw">gJSD</span>(Prob)</code></pre></div>
<pre><code>[1] 0.03512892</code></pre>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
